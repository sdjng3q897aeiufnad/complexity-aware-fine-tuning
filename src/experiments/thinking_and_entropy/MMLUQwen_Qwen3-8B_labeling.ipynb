{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1dc6ac4-22c4-40bb-be37-fdf9634ec465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75259a9b-7d86-446b-9ac9-4153eefa9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May  7 04:01:40 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:27:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              77W / 400W |  19768MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3481      C   python                                    19760MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e36209-6acf-4320-9afe-960dc88295f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c1f433-fb0b-4146-a9a2-5e9b022ed762",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27611491-5425-43e9-a279-1e2eb24fed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "in_path = Path(\"data_qwen3/source/mmlu_pro_stem.tsv\").resolve()\n",
    "\n",
    "out_path = Path(\"data_qwen3/out/qwen3_8b_mmlu_entropy.parquet\").resolve()\n",
    "\n",
    "if os.path.exists(out_path):\n",
    "    df = pd.read_parquet(\n",
    "        out_path,\n",
    "    )\n",
    "else:\n",
    "    df = pd.read_csv(\n",
    "        in_path,\n",
    "        sep=\"\\t\",\n",
    "        header=0,\n",
    "    )\n",
    "    df[\"options\"] = df[\"options\"].apply(ast.literal_eval)\n",
    "    df = df.iloc[:2000]\n",
    "\n",
    "option_ids = [str(i + 1) for i in range(20)]\n",
    "\n",
    "\n",
    "def enumerate_question_and_options(question, options):\n",
    "    options_str = \"\\n\".join([f\"{option_id}. {answer}\".strip() for option_id, answer in zip(option_ids, options)])\n",
    "    user_prompt = f\"Question: {question.strip()}\\nOptions:\\n{options_str}\\nChoose one of the answers. Write down ONLY the NUMBER of the correct answer and nothing else.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930a9b6-8213-47da-80f6-36d94dfd916d",
   "metadata": {},
   "source": [
    "### Answer generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d545ce-3c71-4a54-97a1-c6e0dc24e86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba200b8-051d-4f1a-b354-7499b4939dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, tokenizer, text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        # Layer number (last),  batch size\n",
    "        batch_hidden_states = outputs.hidden_states[-1][0].float()\n",
    "        pool_embeddings = {\n",
    "            \"min\": batch_hidden_states.min(dim=0).values.cpu().numpy().tolist(),\n",
    "            \"max\": batch_hidden_states.max(dim=0).values.cpu().numpy().tolist(),\n",
    "            \"mean\": batch_hidden_states.mean(dim=0).cpu().numpy().tolist(),\n",
    "        }\n",
    "        return pool_embeddings\n",
    "    # TODO: Investigate why it fails for Qwen 3B only for specific rows\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457742b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def compute_entropy_from_logits(logits: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute entropy from logits.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    logits : torch.Tensor\n",
    "        Logits from the model.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Entropy values.\n",
    "    \"\"\"\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    log_probabilities = torch.log(probabilities + 1e-12)\n",
    "    entropy = -torch.sum(probabilities * log_probabilities, dim=-1)\n",
    "    return entropy.item()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LogitSeqStats:\n",
    "    # Generated token selected greedily (no randomness, next token - the most likely one)\n",
    "    greedy_tokens: list[torch.Tensor] = field(default_factory=list)\n",
    "    # List of entropies for every generated token\n",
    "    entropies: list[float] = field(default_factory=list)\n",
    "    # List of raw probabilities for logits with non-zero probabilities for every generated token\n",
    "    every_token_stats: list[list[dict[str, Any]]] = field(default_factory=list)\n",
    "\n",
    "\n",
    "def collect_logit_sequence_stats(logits: list[torch.Tensor]):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    logits : torch.Tensor\n",
    "        Logits for the entire generated sequence. Assumes batch size of 1.\n",
    "        Pass here \"scores\" from \"model.generate\".\n",
    "        Dim: list[tensor(1 x dictionary_size)]\n",
    "    \"\"\"\n",
    "    stats = LogitSeqStats()\n",
    "    for i in range(len(logits)):\n",
    "        # generated token position, batch_dim\n",
    "        token_logits = logits[i][0]\n",
    "        token_entropy = compute_entropy_from_logits(token_logits)\n",
    "        stats.entropies.append(token_entropy)\n",
    "\n",
    "        probabilities = torch.softmax(token_logits, dim=-1)\n",
    "        # Set small cut-off value\n",
    "        mask = probabilities > 1e-5\n",
    "        nonzero_prob_indices = torch.nonzero(mask)\n",
    "        nonzero_probs = probabilities[nonzero_prob_indices]\n",
    "        idx_prob_pairs_list = list(zip(nonzero_prob_indices.cpu().numpy(), nonzero_probs.cpu().numpy()))\n",
    "        position_result = [\n",
    "            {\n",
    "                \"token_idx\": pair[0].item(),\n",
    "                \"token_prob\": pair[1].item(),\n",
    "            }\n",
    "            for pair in idx_prob_pairs_list\n",
    "        ]\n",
    "        stats.every_token_stats.append(position_result)\n",
    "\n",
    "        greedy_token = token_logits.argmax(dim=-1)\n",
    "        stats.greedy_tokens.append(greedy_token)\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e56e311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_correct_mmlu(row, model_answer):\n",
    "    try:\n",
    "        return int(row[\"answer_index\"]) + 1 == int(model_answer.strip())\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71541eb0-5759-49c3-9474-7e1ba4952276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2000 [03:19<35:26:17, 63.88s/it]"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "invalid_answers = 0\n",
    "\n",
    "field_response = \"qwen_8b_response\"\n",
    "field_ans_token_index = \"qwen_8b_ans_token_index\"\n",
    "field_ans_correct = \"qwen_8b_ans_correct\"\n",
    "field_entropies_value = \"qwen_8b_entropies\"\n",
    "field_every_token_info = \"qwen_8b_every_token_info\"\n",
    "field_input_embeddings = \"qwen_8b_input_embeddings\"\n",
    "field_think_embeddings = \"qwen_8b_think_embeddings\"\n",
    "field_answer_embeddings = \"qwen_8b_answer_embeddings\"\n",
    "\n",
    "if field_ans_correct not in df.columns:\n",
    "    df[field_ans_correct] = False\n",
    "if field_entropies_value not in df.columns:\n",
    "    df[field_entropies_value] = \"\"\n",
    "if field_every_token_info not in df.columns:\n",
    "    df[field_every_token_info] = \"\"\n",
    "if field_ans_token_index not in df.columns:\n",
    "    df[field_ans_token_index] = -1\n",
    "if field_response not in df.columns:\n",
    "    df[field_response] = \"\"\n",
    "if field_input_embeddings not in df.columns:\n",
    "    df[field_input_embeddings] = \"\"\n",
    "if field_think_embeddings not in df.columns:\n",
    "    df[field_think_embeddings] = \"\"\n",
    "if field_answer_embeddings not in df.columns:\n",
    "    df[field_answer_embeddings] = \"\"\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    if df.at[index, field_ans_token_index] != -1:\n",
    "        continue\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    sys_prompt = f\"The following are multiple choice questions about {row['base_cluster']}. Write down ONLY the NUMBER of the correct answer and nothing else.\"\n",
    "    user_prompt = enumerate_question_and_options(row[\"question\"], row[\"options\"])\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    max_new_tokens = 5000\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        temperature=0.1,\n",
    "        # top_p=None,\n",
    "        # top_k=None,\n",
    "        # do_sample=False,\n",
    "        num_beams=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    response_raw = outputs.sequences[0, input_length:]\n",
    "    response_decoded = tokenizer.decode(response_raw, skip_special_tokens=True)\n",
    "\n",
    "    df.at[index, field_response] = response_decoded\n",
    "\n",
    "    logit_stats = collect_logit_sequence_stats(outputs.scores)\n",
    "\n",
    "    df.at[index, field_entropies_value] = json.dumps(logit_stats.entropies)\n",
    "    df.at[index, field_every_token_info] = json.dumps(logit_stats.every_token_stats)\n",
    "\n",
    "    think_token_idx = tokenizer.convert_tokens_to_ids(\"</think>\")\n",
    "\n",
    "    think_text = \"\"\n",
    "    answer_text = \"\"\n",
    "    answer_token_idx = -1\n",
    "    for i, token in enumerate(logit_stats.greedy_tokens):\n",
    "        if token == think_token_idx:\n",
    "            answer_token_idx = i + 1\n",
    "            df[field_ans_token_index] = answer_token_idx\n",
    "\n",
    "            think_text = tokenizer.decode(logit_stats.greedy_tokens[:answer_token_idx])\n",
    "            think_embeddings = get_embeddings(model, tokenizer, think_text)\n",
    "            if think_embeddings is not None:\n",
    "                df.at[index, field_think_embeddings] = json.dumps(think_embeddings)\n",
    "\n",
    "            answer_text = tokenizer.decode(logit_stats.greedy_tokens[answer_token_idx:], skip_special_tokens=True)\n",
    "            answer_text = answer_text.strip()\n",
    "            answer_embeddings = get_embeddings(model, tokenizer, answer_text)\n",
    "            if answer_embeddings:\n",
    "                df.at[index, field_answer_embeddings] = json.dumps(answer_embeddings)\n",
    "\n",
    "            break\n",
    "\n",
    "    input_embeddings = get_embeddings(model, tokenizer, formatted_prompt)\n",
    "    if input_embeddings:\n",
    "        df.at[index, field_input_embeddings] = json.dumps(input_embeddings)\n",
    "\n",
    "    if answer_text in option_ids:\n",
    "        # print(f\"loop {index} -> after entropy: {model.get_memory_footprint(return_buffers=True) / 10**9} GB\")\n",
    "        df.at[index, field_ans_correct] = check_answer_correct_mmlu(row, answer_text)\n",
    "    else:\n",
    "        invalid_answers += 1\n",
    "\n",
    "    if index % 500 == 0:\n",
    "        df.to_parquet(out_path, compression=\"gzip\")\n",
    "\n",
    "df.to_parquet(out_path, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450465f2-b802-430e-a0b0-2eb3d17078e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
