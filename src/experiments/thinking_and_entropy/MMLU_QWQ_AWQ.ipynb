{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75259a9b-7d86-446b-9ac9-4153eefa9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 22 11:17:16 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:51:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              77W / 400W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e36209-6acf-4320-9afe-960dc88295f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c1f433-fb0b-4146-a9a2-5e9b022ed762",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "MODEL_NAME = \"Qwen/QwQ-32B-AWQ\"\n",
    "SAVE_DATA_PATH = Path(\"QWQ_labeling\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27611491-5425-43e9-a279-1e2eb24fed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"mmlu_pro_stem.tsv\"\n",
    "data_path = os.path.join(DATA_PATH, data_name)\n",
    "\n",
    "df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "\n",
    "df[\"options\"] = df[\"options\"].apply(ast.literal_eval)\n",
    "df[\"answer_index\"] = df[\"answer_index\"].apply(lambda x: str(x + 1))\n",
    "\n",
    "def enumerate_question_and_options(line):\n",
    "    enumerated_variants = \"\\n\".join(\n",
    "        f\"{i + 1}) {option}\" for i, option in enumerate(line[\"options\"])\n",
    "    )\n",
    "    return f\"{line['question']}\\n\\n{enumerated_variants}\"\n",
    "\n",
    "df[\"question_with_variants\"] = df.apply(enumerate_question_and_options, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce619a28-6a97-4b92-aa8d-5ac02daf6586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which of the following criticisms of Llewellyn's distinction between the grand and formal styles of legal reasoning is the most compelling?\n",
      "\n",
      "1) There is no distinction between the two forms of legal reasoning.\n",
      "2) Judges are appointed to interpret the law, not to make it.\n",
      "3) It is misleading to pigeon-hole judges in this way.\n",
      "4) Judicial reasoning is always formal.\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(df.iloc[idx].question_with_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930a9b6-8213-47da-80f6-36d94dfd916d",
   "metadata": {},
   "source": [
    "### Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d545ce-3c71-4a54-97a1-c6e0dc24e86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:12<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba200b8-051d-4f1a-b354-7499b4939dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    last_hidden_state = outputs.hidden_states[-1]\n",
    "    pool_embeddings = {\n",
    "        \"min_pooling\": last_hidden_state.min(dim=1).values[0].cpu().numpy(),\n",
    "        \"max_pooling\": last_hidden_state.max(dim=1).values[0].cpu().numpy(),\n",
    "        \"mean_pooling\": last_hidden_state.mean(dim=1)[0].cpu().numpy(),\n",
    "    }\n",
    "    return pool_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71541eb0-5759-49c3-9474-7e1ba4952276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12032 [00:00<?, ?it/s]From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "  0%|          | 11/12032 [1:41:19<1342:36:15, 402.08s/it]"
     ]
    }
   ],
   "source": [
    "for idx in tqdm(range(df.shape[0])):\n",
    "    new_foler_path = os.path.join(SAVE_DATA_PATH, str(idx))\n",
    "    if os.path.exists(new_foler_path):\n",
    "        shutil.rmtree(new_foler_path)\n",
    "    os.mkdir(new_foler_path)\n",
    "    \n",
    "    row = df.iloc[idx]\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert in the field of {row[\"category\"]}. Answer the questions.\n",
    "Choose one of the answers. Write down ONLY the NUMBER of the correct answer and nothing else.\n",
    "---\n",
    "{row[\"question_with_variants\"]}\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    max_new_tokens = 32768\n",
    "    output = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    entropies = []\n",
    "    for step, scores in enumerate(output.scores):\n",
    "        probs = F.softmax(scores, dim=-1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=-1)\n",
    "        entropies.append(entropy.item())\n",
    "        \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):]\n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, output.sequences)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    think_text, answer_text = [part.strip() for part in response.split(\"</think>\")]\n",
    "    \n",
    "    think_token_idx = tokenizer.convert_tokens_to_ids(\"</think>\")\n",
    "    \n",
    "    generation_probabilistic_info = dict()\n",
    "    \n",
    "    every_token_info = list()\n",
    "    for gen_token_position, generated_token in enumerate(generated_ids[0].cpu().numpy()):\n",
    "        token_logits = output.scores[gen_token_position][0]\n",
    "        probs = F.softmax(token_logits, dim=-1)\n",
    "        mask = probs != 0\n",
    "        nonzero_prob_indices = torch.nonzero(mask).squeeze(dim=0)\n",
    "        nonzero_probs = probs[nonzero_prob_indices]\n",
    "        idx_prob_pairs_list = list(zip(nonzero_prob_indices.cpu().numpy(), nonzero_probs.cpu().numpy()))\n",
    "        position_result = [\n",
    "            {\n",
    "                \"token_idx\": pair[0].item(),\n",
    "                \"token_prob\": pair[1].item(),\n",
    "            }\n",
    "            for pair in idx_prob_pairs_list\n",
    "        ]\n",
    "        \n",
    "        every_token_info.append(position_result)\n",
    "    \n",
    "        if generated_token == think_token_idx:\n",
    "            generation_probabilistic_info[\"think_token_idx\"] = gen_token_position\n",
    "    \n",
    "    generation_probabilistic_info[\"every_token_info\"] = every_token_info\n",
    "    generation_probabilistic_info[\"generated_token_num\"] = gen_token_position + 1\n",
    "\n",
    "    \n",
    "    with open(os.path.join(new_foler_path, \"generation_probabilistic_info.json\"), \"w\") as f:\n",
    "        json.dump(generation_probabilistic_info, f, indent=2)\n",
    "\n",
    "    input_embeddings = get_embedding(prompt)\n",
    "    np.savez(\n",
    "        os.path.join(new_foler_path, \"input_embeddings.npz\"),\n",
    "        **input_embeddings\n",
    "    )\n",
    "\n",
    "    think_embeddings = get_embedding(think_text)\n",
    "    np.savez(\n",
    "        os.path.join(new_foler_path, \"think_embeddings.npz\"),\n",
    "        **think_embeddings\n",
    "    )\n",
    "\n",
    "    answer_embeddings = get_embedding(answer_text)\n",
    "    np.savez(\n",
    "        os.path.join(new_foler_path, \"answer_embeddings.npz\"),\n",
    "        **answer_embeddings\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6a67a-088b-4cdf-82e9-78f619a44dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
