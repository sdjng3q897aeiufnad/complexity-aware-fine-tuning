{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75259a9b-7d86-446b-9ac9-4153eefa9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 22 22:37:12 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:27:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              79W / 400W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e36209-6acf-4320-9afe-960dc88295f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1f433-fb0b-4146-a9a2-5e9b022ed762",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27611491-5425-43e9-a279-1e2eb24fed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "in_path = Path(\"../../data/source/mmlu_pro_stem.tsv\").resolve()\n",
    "\n",
    "out_path = Path(\"../../data/out/r1_7b_mmlu_entropy.parquet\").resolve()\n",
    "\n",
    "if os.path.exists(out_path):\n",
    "    df = pd.read_parquet(\n",
    "        out_path,\n",
    "    )\n",
    "else:\n",
    "    df = pd.read_csv(\n",
    "        in_path,\n",
    "        sep=\"\\t\",\n",
    "        header=0,\n",
    "    )\n",
    "\n",
    "df[\"options\"] = df[\"options\"].apply(ast.literal_eval)\n",
    "\n",
    "option_ids = [str(i + 1) for i in range(20)]\n",
    "\n",
    "\n",
    "def enumerate_question_and_options(question, options):\n",
    "    options_str = \"\\n\".join([f\"{option_id}. {answer}\".strip() for option_id, answer in zip(option_ids, options)])\n",
    "    user_prompt = f\"Question: {question.strip()}\\nOptions:\\n{options_str}\\nChoose one of the answers. Write down ONLY the NUMBER of the correct answer and nothing else.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930a9b6-8213-47da-80f6-36d94dfd916d",
   "metadata": {},
   "source": [
    "### Answer generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d545ce-3c71-4a54-97a1-c6e0dc24e86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f559fe419e814298a960f15c2649c3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba200b8-051d-4f1a-b354-7499b4939dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    # Layer number (last),  batch size\n",
    "    batch_hidden_states = outputs.hidden_states[-1][0].float()\n",
    "    pool_embeddings = {\n",
    "        \"min\": batch_hidden_states.min(dim=0).values.cpu().numpy().tolist(),\n",
    "        \"max\": batch_hidden_states.max(dim=0).values.cpu().numpy().tolist(),\n",
    "        \"mean\": batch_hidden_states.mean(dim=0).cpu().numpy().tolist(),\n",
    "    }\n",
    "    return pool_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457742b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def compute_entropy_from_logits(logits: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute entropy from logits.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    logits : torch.Tensor\n",
    "        Logits from the model.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Entropy values.\n",
    "    \"\"\"\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    log_probabilities = torch.log(probabilities + 1e-12)\n",
    "    entropy = -torch.sum(probabilities * log_probabilities, dim=-1)\n",
    "    return entropy.item()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LogitSeqStats:\n",
    "    # Generated token selected greedily (no randomness, next token - the most likely one)\n",
    "    greedy_tokens: list[torch.Tensor] = field(default_factory=list)\n",
    "    # List of entropies for every generated token\n",
    "    entropies: list[float] = field(default_factory=list)\n",
    "    # List of raw probabilities for logits with non-zero probabilities for every generated token\n",
    "    every_token_stats: list[list[dict[str, Any]]] = field(default_factory=list)\n",
    "\n",
    "\n",
    "def collect_logit_sequence_stats(logits: list[torch.Tensor]):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    logits : torch.Tensor\n",
    "        Logits for the entire generated sequence. Assumes batch size of 1.\n",
    "        Pass here \"scores\" from \"model.generate\".\n",
    "        Dim: list[tensor(1 x dictionary_size)]\n",
    "    \"\"\"\n",
    "    stats = LogitSeqStats()\n",
    "    for i in range(len(logits)):\n",
    "        # generated token position, batch_dim\n",
    "        token_logits = logits[i][0]\n",
    "        token_entropy = compute_entropy_from_logits(token_logits)\n",
    "        stats.entropies.append(token_entropy)\n",
    "\n",
    "        probabilities = torch.softmax(token_logits, dim=-1)\n",
    "        # Set small cut-off value\n",
    "        mask = probabilities > 1e-5\n",
    "        nonzero_prob_indices = torch.nonzero(mask)\n",
    "        nonzero_probs = probabilities[nonzero_prob_indices]\n",
    "        idx_prob_pairs_list = list(zip(nonzero_prob_indices.cpu().numpy(), nonzero_probs.cpu().numpy()))\n",
    "        position_result = [\n",
    "            {\n",
    "                \"token_idx\": pair[0].item(),\n",
    "                \"token_prob\": pair[1].item(),\n",
    "            }\n",
    "            for pair in idx_prob_pairs_list\n",
    "        ]\n",
    "        stats.every_token_stats.append(position_result)\n",
    "\n",
    "        greedy_token = token_logits.argmax(dim=-1)\n",
    "        stats.greedy_tokens.append(greedy_token)\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_correct_mmlu(row, model_answer):\n",
    "    try:\n",
    "        return int(row[\"answer_index\"]) + 1 == int(model_answer.strip())\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71541eb0-5759-49c3-9474-7e1ba4952276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12024 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 1/12024 [00:25<84:31:43, 25.31s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 2/12024 [00:57<97:18:15, 29.14s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 3/12024 [01:29<102:49:26, 30.79s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 4/12024 [02:51<170:19:31, 51.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 5/12024 [03:03<123:06:25, 36.87s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 6/12024 [03:21<101:22:52, 30.37s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 7/12024 [04:07<118:39:05, 35.55s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 8/12024 [04:33<108:44:20, 32.58s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 9/12024 [05:01<103:26:57, 31.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 10/12024 [05:12<82:55:03, 24.85s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 11/12024 [05:25<70:57:40, 21.27s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 12/12024 [05:37<61:51:00, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 13/12024 [05:52<57:54:56, 17.36s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 14/12024 [06:06<54:28:34, 16.33s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "  0%|          | 15/12024 [06:11<42:50:55, 12.85s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "invalid_answers = 0\n",
    "\n",
    "field_response = \"r1_7b_response\"\n",
    "field_ans_token_index = \"r1_7b_ans_token_index\"\n",
    "field_ans_correct = \"r1_7b_ans_correct\"\n",
    "field_entropies_value = \"r1_7b_entropies\"\n",
    "field_every_token_info = \"r1_7b_every_token_info\"\n",
    "field_input_embeddings = \"r1_7b_input_embeddings\"\n",
    "field_think_embeddings = \"r1_7b_think_embeddings\"\n",
    "field_answer_embeddings = \"r1_7b_answer_embeddings\"\n",
    "\n",
    "if field_ans_correct not in df.columns:\n",
    "    df[field_ans_correct] = False\n",
    "if field_entropies_value not in df.columns:\n",
    "    df[field_entropies_value] = \"\"\n",
    "if field_every_token_info not in df.columns:\n",
    "    df[field_every_token_info] = \"\"\n",
    "if field_ans_token_index not in df.columns:\n",
    "    df[field_ans_token_index] = -1\n",
    "if field_response not in df.columns:\n",
    "    df[field_response] = \"\"\n",
    "if field_input_embeddings not in df.columns:\n",
    "    df[field_input_embeddings] = \"\"\n",
    "if field_think_embeddings not in df.columns:\n",
    "    df[field_think_embeddings] = \"\"\n",
    "if field_answer_embeddings not in df.columns:\n",
    "    df[field_answer_embeddings] = \"\"\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    if df.at[index, field_ans_token_index] != -1:\n",
    "        continue\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    sys_prompt = f\"The following are multiple choice questions about {row['base_cluster']}. Write down ONLY the NUMBER of the correct answer and nothing else.\"\n",
    "    user_prompt = enumerate_question_and_options(row[\"question\"], row[\"options\"])\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    max_new_tokens = 32768\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "        top_k=None,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    response_raw = outputs.sequences[0, input_length:]\n",
    "    response_decoded = tokenizer.decode(response_raw, skip_special_tokens=True)\n",
    "\n",
    "    df.at[index, field_response] = response_decoded\n",
    "\n",
    "    logit_stats = collect_logit_sequence_stats(outputs.scores)\n",
    "\n",
    "    df.at[index, field_entropies_value] = json.dumps(logit_stats.entropies)\n",
    "    df.at[index, field_every_token_info] = json.dumps(logit_stats.every_token_stats)\n",
    "\n",
    "    think_token_idx = tokenizer.convert_tokens_to_ids(\"</think>\")\n",
    "\n",
    "    think_text = \"\"\n",
    "    answer_text = \"\"\n",
    "    answer_token_idx = -1\n",
    "    for i, token in enumerate(logit_stats.greedy_tokens):\n",
    "        if token == think_token_idx:\n",
    "            answer_token_idx = i + 1\n",
    "\n",
    "            think_text = tokenizer.decode(logit_stats.greedy_tokens[:answer_token_idx])\n",
    "            think_embeddings = get_embeddings(model, tokenizer, think_text)\n",
    "            df.at[index, field_think_embeddings] = json.dumps(think_embeddings)\n",
    "\n",
    "            answer_text = tokenizer.decode(logit_stats.greedy_tokens[answer_token_idx:], skip_special_tokens=True)\n",
    "            answer_text = answer_text.strip()\n",
    "            answer_embeddings = get_embeddings(model, tokenizer, answer_text)\n",
    "            df.at[index, field_answer_embeddings] = json.dumps(answer_embeddings)\n",
    "\n",
    "            break\n",
    "\n",
    "    input_embeddings = get_embeddings(model, tokenizer, formatted_prompt)\n",
    "    df.at[index, field_input_embeddings] = json.dumps(input_embeddings)\n",
    "\n",
    "    if answer_text in option_ids:\n",
    "        # print(f\"loop {index} -> after entropy: {model.get_memory_footprint(return_buffers=True) / 10**9} GB\")\n",
    "        df.at[index, field_ans_correct] = check_answer_correct_mmlu(row, answer_text)\n",
    "    else:\n",
    "        invalid_answers += 1\n",
    "\n",
    "    if index % 10 == 0:\n",
    "        df.to_parquet(out_path, compression=\"gzip\")\n",
    "\n",
    "df.to_parquet(out_path, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7a858-4a13-447f-adab-7e3cd2fd3e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
